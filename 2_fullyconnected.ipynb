{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2_fullyconnected.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"kR-4eNdK6lYS","colab_type":"text"},"cell_type":"markdown","source":["Deep Learning\n","=============\n","\n","Assignment 2\n","------------\n","\n","Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n","\n","The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."]},{"metadata":{"id":"JLpLa8Jt7Vu4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle\n","from six.moves import range"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1HrCK6e17WzV","colab_type":"text"},"cell_type":"markdown","source":["First reload the data we generated in `1_notmnist.ipynb`."]},{"metadata":{"id":"tyEjEEMj8Wg1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4}],"base_uri":"https://localhost:8080/","height":130},"outputId":"c050bb4a-41eb-49cb-9640-3b0a2846d0c9","executionInfo":{"status":"ok","timestamp":1522955695463,"user_tz":-180,"elapsed":22840,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"metadata":{"id":"f8j8NJocORH0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":44},"outputId":"e4342152-9f3c-485f-cbce-02375721d9d8","executionInfo":{"status":"ok","timestamp":1522955726302,"user_tz":-180,"elapsed":1464,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["tf.test.gpu_device_name()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"xLm6yl-l8ZFb","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!mkdir drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UGQJcYyq8zku","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":79},"outputId":"621a2bdc-38c5-4271-ae7e-feaed20cdbd5","executionInfo":{"status":"ok","timestamp":1522955707571,"user_tz":-180,"elapsed":3473,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["!ls drive/ColabUtils/udacity"],"execution_count":6,"outputs":[{"output_type":"stream","text":["1_notmnist.ipynb\t4_convolutions.ipynb  Dockerfile\r\n","2_fullyconnected.ipynb\t5_word2vec.ipynb      notMNIST.pickle\r\n","3_regularization.ipynb\t6_lstm.ipynb\t      README.md\r\n"],"name":"stdout"}]},{"metadata":{"id":"y3-cj1bpmuxc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":79},"cellView":"both","outputId":"98fda722-b454-49b5-ced3-c762bbb2971b","executionInfo":{"status":"ok","timestamp":1522955748345,"user_tz":-180,"elapsed":16194,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["pickle_file = 'drive/ColabUtils/udacity/notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"metadata":{"id":"L7aHrm6nGDMB","colab_type":"text"},"cell_type":"markdown","source":["Reformat into a shape that's more adapted to the models we're going to train:\n","- data as a flat matrix,\n","- labels as float 1-hot encodings."]},{"metadata":{"id":"IRSyYiIIGIzS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":89},"cellView":"both","outputId":"31620d10-3504-41e4-ee49-428907c6b2fc","executionInfo":{"status":"ok","timestamp":1522955762755,"user_tz":-180,"elapsed":1329,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["image_size = 28\n","num_labels = 10\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n","  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Training set (200000, 784) (200000, 10)\n","Validation set (10000, 784) (10000, 10)\n","Test set (10000, 784) (10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"nCLVqyQ5vPPH","colab_type":"text"},"cell_type":"markdown","source":["We're first going to train a multinomial logistic regression using simple gradient descent.\n","\n","TensorFlow works like this:\n","* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n","\n","      with graph.as_default():\n","          ...\n","\n","* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n","\n","      with tf.Session(graph=graph) as session:\n","          ...\n","\n","Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"]},{"metadata":{"id":"Nfv39qvtvOl_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":199},"cellView":"both","outputId":"3703256b-83e2-4b22-b44d-4b8b333ce566","executionInfo":{"status":"ok","timestamp":1522949319216,"user_tz":-180,"elapsed":808,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["# With gradient descent training, even this much data is prohibitive.\n","# Subset the training data for faster turnaround.\n","train_subset = 10000\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data.\n","  # Load the training, validation and test data into constants that are\n","  # attached to the graph.\n","  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n","  tf_train_labels = tf.constant(train_labels[:train_subset])\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  # These are the parameters that we are going to be training. The weight\n","  # matrix will be initialized using random values following a (truncated)\n","  # normal distribution. The biases get initialized to zero.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  # We multiply the inputs with the weight matrix, and add biases. We compute\n","  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n","  # it's very common, and it can be optimized). We take the average of this\n","  # cross-entropy across all training examples: that's our loss.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  # We are going to find the minimum of this loss using gradient descent.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  # These are not part of training, but merely here so that we can report\n","  # accuracy figures as we train.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-7-58acd45c85e4>:29: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See tf.nn.softmax_cross_entropy_with_logits_v2.\n","\n"],"name":"stdout"}]},{"metadata":{"id":"KQcL4uqISHjP","colab_type":"text"},"cell_type":"markdown","source":["Let's run this computation and iterate:"]},{"metadata":{"id":"z2cjdenH869W","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":10}],"base_uri":"https://localhost:8080/","height":538},"cellView":"both","outputId":"51941965-0e03-48e5-9dab-ef9381b9c160","executionInfo":{"status":"ok","timestamp":1522949352511,"user_tz":-180,"elapsed":32201,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["num_steps = 801\n","\n","def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])\n","\n","with tf.Session(graph=graph) as session:\n","  # This is a one-time operation which ensures the parameters get initialized as\n","  # we described in the graph: random weights for the matrix, zeros for the\n","  # biases. \n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    # Run the computations. We tell .run() that we want to run the optimizer,\n","    # and get the loss value and the training predictions returned as numpy\n","    # arrays.\n","    _, l, predictions = session.run([optimizer, loss, train_prediction])\n","    if (step % 100 == 0):\n","      print('Loss at step %d: %f' % (step, l))\n","      print('Training accuracy: %.1f%%' % accuracy(\n","        predictions, train_labels[:train_subset, :]))\n","      # Calling .eval() on valid_prediction is basically like calling run(), but\n","      # just to get that one numpy array. Note that it recomputes all its graph\n","      # dependencies.\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Initialized\n","Loss at step 0: 15.653460\n","Training accuracy: 10.9%\n","Validation accuracy: 13.1%\n","Loss at step 100: 2.321112\n","Training accuracy: 71.2%\n","Validation accuracy: 71.1%\n","Loss at step 200: 1.869491\n","Training accuracy: 74.2%\n","Validation accuracy: 73.7%\n","Loss at step 300: 1.624902\n","Training accuracy: 75.7%\n","Validation accuracy: 74.7%\n","Loss at step 400: 1.456742\n","Training accuracy: 76.8%\n","Validation accuracy: 75.2%\n","Loss at step 500: 1.329738\n","Training accuracy: 77.3%\n","Validation accuracy: 75.7%\n","Loss at step 600: 1.229237\n","Training accuracy: 78.0%\n","Validation accuracy: 75.9%\n","Loss at step 700: 1.147428\n","Training accuracy: 78.7%\n","Validation accuracy: 76.2%\n","Loss at step 800: 1.079410\n","Training accuracy: 79.2%\n","Validation accuracy: 76.2%\n","Test accuracy: 82.8%\n"],"name":"stdout"}]},{"metadata":{"id":"x68f-hxRGm3H","colab_type":"text"},"cell_type":"markdown","source":["Let's now switch to stochastic gradient descent training instead, which is much faster.\n","\n","The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."]},{"metadata":{"id":"qhPMzWYRGrzM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["batch_size = 128\n","\n","graph = tf.Graph()\n","with graph.as_default():\n","\n","  # Input data. For the training data, we use a placeholder that will be fed\n","  # at run time with a training minibatch.\n","  tf_train_dataset = tf.placeholder(tf.float32,\n","                                    shape=(batch_size, image_size * image_size))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  weights = tf.Variable(\n","    tf.truncated_normal([image_size * image_size, num_labels]))\n","  biases = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  logits = tf.matmul(tf_train_dataset, weights) + biases\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    tf.matmul(tf_valid_dataset, weights) + biases)\n","  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XmVZESmtG4JH","colab_type":"text"},"cell_type":"markdown","source":["Let's run it:"]},{"metadata":{"id":"tjiiELLPWHpu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["for step in range(10):\n","  print ((step * batch_size) % (train_labels.shape[0] - batch_size))\n","train_labels.shape, batch_size, train_labels.shape[0] - batch_size"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FoF91pknG_YW","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4}],"base_uri":"https://localhost:8080/","height":289},"cellView":"both","outputId":"53c5c2b8-0fdf-4dc2-e7d9-84b6c830aa11","executionInfo":{"status":"ok","timestamp":1522949360457,"user_tz":-180,"elapsed":4760,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["num_steps = 3001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print(\"Initialized\")\n","  for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 1000 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0: 13.456728\n","Minibatch accuracy: 6.2%\n","Validation accuracy: 16.5%\n","Minibatch loss at step 1000: 1.153790\n","Minibatch accuracy: 78.9%\n","Validation accuracy: 77.1%\n","Minibatch loss at step 2000: 0.966280\n","Minibatch accuracy: 78.1%\n","Validation accuracy: 78.5%\n","Minibatch loss at step 3000: 0.735428\n","Minibatch accuracy: 85.2%\n","Validation accuracy: 78.9%\n","Test accuracy: 86.4%\n"],"name":"stdout"}]},{"metadata":{"id":"7omWxtvLLxik","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem\n","-------\n","\n","Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n","\n","---"]},{"metadata":{"id":"MFzXcNieZL83","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# http://pytorch.org/\n","from os import path\n","from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","\n","accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n","\n","!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n","import torch"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sRszqWQaZ5Zc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import torch.nn as nn\n","from torch.autograd import Variable as V"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iwpdfTsla2_7","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["class LogisticRegression(nn.Module):\n","  def __init__(self, input_size, hid, num_classes):\n","    super(LogisticRegression, self).__init__()\n","    self.l1 = nn.Linear(input_size, hid)\n","    self.l2 = nn.Linear(hid, num_classes)\n","    self.relu = nn.ReLU()\n","#     self.softmax = nn.Softmax()\n","  def forward(self, x):\n","    relu = self.relu(self.l1(x))\n","    out = self.l2(relu)\n","    return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"un5p_TmGb_Fj","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["model = LogisticRegression(image_size * image_size, 1024, num_labels).float()\n","criterion = nn.CrossEntropyLoss()  \n","optimizer = torch.optim.SGD(model.parameters(), lr=.5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l1Ih9lyOpPbB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":10},"outputId":"930200c4-d07d-4821-e46b-1a9cb54d0723","executionInfo":{"status":"ok","timestamp":1522949605694,"user_tz":-180,"elapsed":1004,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["train_labels_ind = np.array([ np.where(r==1)[0][0] for r in train_labels ])\n","train_labels_ind"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4, 9, 6, ..., 2, 4, 4])"]},"metadata":{"tags":[]},"execution_count":21}]},{"metadata":{"id":"Eh2NoSib4-w6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def onehot2ind(x): return np.array([ np.where(r==1)[0][0] for r in x ], dtype=np.long)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dG-LZrIN3uDE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":10},"outputId":"4910d1c9-e91d-424c-8d86-897e23f033b6","executionInfo":{"status":"ok","timestamp":1522949618797,"user_tz":-180,"elapsed":816,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["train_labels_ind.shape"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(200000,)"]},"metadata":{"tags":[]},"execution_count":22}]},{"metadata":{"id":"qs1c1V9yeYhu","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":31}],"base_uri":"https://localhost:8080/","height":10},"outputId":"4ac01ad7-e6e5-4efd-f037-67b574a24c34","executionInfo":{"status":"ok","timestamp":1522949721526,"user_tz":-180,"elapsed":37727,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["for step in range(num_steps):\n","  offstep = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","  batch_data = train_dataset[offset:(offset + batch_size), :]\n","  batch_labels = train_labels_ind[offset:(offset + batch_size)]\n","  images = V(torch.Tensor(batch_data))\n","  labels = V(torch.Tensor(batch_labels)).long()\n","\n","  # Forward + Backward + Optimize\n","  optimizer.zero_grad()\n","  outputs = model(images)\n","  loss = criterion(outputs, labels)\n","  loss_val = criterion()\n","  loss.backward()\n","  optimizer.step()\n","\n","  if (step) % 100 == 0:\n","      print ('step: %d, Loss: %.4f' \n","             % (step, loss.data[0]))\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["step: 0, Loss: 2.3111\n","step: 100, Loss: 0.0076\n","step: 200, Loss: 0.0031\n","step: 300, Loss: 0.0019\n","step: 400, Loss: 0.0013\n","step: 500, Loss: 0.0010\n","step: 600, Loss: 0.0008\n","step: 700, Loss: 0.0007\n","step: 800, Loss: 0.0006\n","step: 900, Loss: 0.0005\n","step: 1000, Loss: 0.0004\n","step: 1100, Loss: 0.0004\n","step: 1200, Loss: 0.0004\n","step: 1300, Loss: 0.0003\n","step: 1400, Loss: 0.0003\n","step: 1500, Loss: 0.0003\n","step: 1600, Loss: 0.0003\n","step: 1700, Loss: 0.0002\n","step: 1800, Loss: 0.0002\n","step: 1900, Loss: 0.0002\n","step: 2000, Loss: 0.0002\n","step: 2100, Loss: 0.0002\n","step: 2200, Loss: 0.0002\n","step: 2300, Loss: 0.0002\n","step: 2400, Loss: 0.0002\n","step: 2500, Loss: 0.0002\n","step: 2600, Loss: 0.0002\n","step: 2700, Loss: 0.0001\n","step: 2800, Loss: 0.0001\n","step: 2900, Loss: 0.0001\n","step: 3000, Loss: 0.0001\n"],"name":"stdout"}]},{"metadata":{"id":"PxnZJCFF43gF","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":10},"outputId":"aa0662bd-0ab8-466d-fe5b-846a6b16c013","executionInfo":{"status":"ok","timestamp":1522949924782,"user_tz":-180,"elapsed":831,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["valid_dataset.shape, valid_labels.shape"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((10000, 784), (10000, 10))"]},"metadata":{"tags":[]},"execution_count":25}]},{"metadata":{"id":"zjXF1SSg5PrV","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":10},"outputId":"c7e1adfd-68a8-4180-e8aa-842050e2501c","executionInfo":{"status":"ok","timestamp":1522950094713,"user_tz":-180,"elapsed":657,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["valid_labels_ind = onehot2ind(valid_labels)\n","valid_labels_ind.shape"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10000,)"]},"metadata":{"tags":[]},"execution_count":31}]},{"metadata":{"id":"ggn5HVE89h25","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":10},"outputId":"b860dc6a-bbb3-4bd0-a916-1c056245e2f0","executionInfo":{"status":"ok","timestamp":1522951196284,"user_tz":-180,"elapsed":629,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["valid_labels_ind.size"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":34}]},{"metadata":{"id":"XtO6HSDK-NQH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"OAepL9o64rC2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":6},{"item_id":7}],"base_uri":"https://localhost:8080/","height":10},"outputId":"febd10db-3283-4fad-8f3d-7a3caa13890f","executionInfo":{"status":"error","timestamp":1522951548465,"user_tz":-180,"elapsed":1795,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["correct = 0\n","total = 0\n","for step in range(len(valid_labels_ind)):\n","    images = V(torch.Tensor(valid_dataset))\n","    outputs = model(images)\n","    _, predicted = torch.max(outputs.data, 1)\n","    total += valid_labels_ind.size\n","    correct += (predicted == torch.Tensor(valid_labels_ind).long()).sum()\n","      print('Accuracy of the model on the [%d] test images: %d %%' % (total, (100 * correct / total)))"],"execution_count":45,"outputs":[{"output_type":"stream","text":["Accuracy of the model on the [630000] test images: 73 %\n","Accuracy of the model on the [640000] test images: 73 %\n","Accuracy of the model on the [650000] test images: 73 %\n","Accuracy of the model on the [660000] test images: 73 %\n","Accuracy of the model on the [670000] test images: 73 %\n","Accuracy of the model on the [680000] test images: 73 %\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-05142d3baba0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvalid_labels_ind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_labels_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"7n1zqCQLkbqJ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["for step in range(num_steps):\n","    # Pick an offset within the training data, which has been randomized.\n","    # Note: we could use better randomization across epochs.\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    # Generate a minibatch.\n","    batch_data = train_dataset[offset:(offset + batch_size), :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    # Prepare a dictionary telling the session where to feed the minibatch.\n","    # The key of the dictionary is the placeholder node of the graph to be fed,\n","    # and the value is the numpy array to feed to it.\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 1000 == 0):\n","      print(\"Minibatch loss at step %d: %f\" % (step, l))\n","      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n","      print(\"Validation accuracy: %.1f%%\" % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SUkKYLGX_Ldl","colab_type":"text"},"cell_type":"markdown","source":["---\n","#Problem\n","Same with tf\n","\n","---"]},{"metadata":{"id":"WRpfV48IKWpa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":64},"outputId":"ceead257-81da-426a-abd6-36ac12123118","executionInfo":{"status":"ok","timestamp":1522954500189,"user_tz":-180,"elapsed":644,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["num_labels"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":49}]},{"metadata":{"id":"UskQeSF7K1B_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":64},"outputId":"d1fd2c91-272d-450a-e796-abda08aa4dac","executionInfo":{"status":"ok","timestamp":1522954626891,"user_tz":-180,"elapsed":635,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["tf_train_dataset"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor 'Const:0' shape=(10000, 784) dtype=float32>"]},"metadata":{"tags":[]},"execution_count":50}]},{"metadata":{"id":"93yHXxSN_LBB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":220},"outputId":"d46b3baf-f709-44c1-a98b-08ab93457650","executionInfo":{"status":"ok","timestamp":1522955776869,"user_tz":-180,"elapsed":1712,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["train_subset = 10000\n","\n","graph = tf.Graph()\n","\n","num_hidden = 1024\n","\n","def predict(x, w1, b1, w2, b2):\n","  lin1 = tf.matmul(x, w1) + b1\n","  lin2 = tf.matmul(tf.nn.relu(lin1), w2) + b2\n","  return lin2\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  # Load the training, validation and test data into constants that are\n","  # attached to the graph.\n","  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n","  tf_train_labels = tf.constant(train_labels[:train_subset])\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n"," \n","  # Variables.\n","  # These are the parameters that we are going to be training. The weight\n","  # matrix will be initialized using random values following a (truncated)\n","  # normal distribution. The biases get initialized to zero.\n","  w1 = tf.Variable(\n","      tf.truncated_normal([image_size * image_size, num_hidden]))\n","  b1 = tf.Variable(tf.zeros([num_hidden]))\n","  \n","  w2 = tf.Variable(tf.truncated_normal([num_hidden, num_labels]))\n","  b2 = tf.Variable(tf.zeros([num_labels]))\n","  \n","  # Training computation.\n","  # We multiply the inputs with the weight matrix, and add biases. We compute\n","  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n","  # it's very common, and it can be optimized). We take the average of this\n","  # cross-entropy across all training examples: that's our loss.\n","  logits = predict(tf_train_dataset, w1, b1, w2, b2)\n","  \n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","  \n","  # Optimizer.\n","  # We are going to find the minimum of this loss using gradient descent.\n","  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  # These are not part of training, but merely here so that we can report\n","  # accuracy figures as we train.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(\n","    predict(tf_valid_dataset, w1, b1, w2, b2))\n","  test_prediction = tf.nn.softmax(predict(tf_test_dataset, w1, b1, w2, b2))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-10-19b3fbe769d8>:42: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See tf.nn.softmax_cross_entropy_with_logits_v2.\n","\n"],"name":"stdout"}]},{"metadata":{"id":"6ehR-69PPSuD","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["num_steps = 801"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FR5YMQviPf0Z","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BewASSRGMery","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":10}],"base_uri":"https://localhost:8080/","height":528},"outputId":"6806a96b-ad1c-4597-ae64-06ab475b20bf","executionInfo":{"status":"ok","timestamp":1522955870891,"user_tz":-180,"elapsed":16926,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["with tf.Session(graph=graph) as session:\n","  # This is a one-time operation which ensures the parameters get initialized as\n","  # we described in the graph: random weights for the matrix, zeros for the\n","  # biases. \n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    # Run the computations. We tell .run() that we want to run the optimizer,\n","    # and get the loss value and the training predictions returned as numpy\n","    # arrays.\n","    _, l, predictions = session.run([optimizer, loss, train_prediction])\n","    if (step % 100 == 0):\n","      print('Loss at step %d: %f' % (step, l))\n","      print('Training accuracy: %.1f%%' % accuracy(\n","        predictions, train_labels[:train_subset, :]))\n","      # Calling .eval() on valid_prediction is basically like calling run(), but\n","      # just to get that one numpy array. Note that it recomputes all its graph\n","      # dependencies.\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Initialized\n","Loss at step 0: 295.685791\n","Training accuracy: 11.3%\n","Validation accuracy: 35.6%\n","Loss at step 100: 2.901247\n","Training accuracy: 93.0%\n","Validation accuracy: 79.7%\n","Loss at step 200: 0.298337\n","Training accuracy: 98.4%\n","Validation accuracy: 79.5%\n","Loss at step 300: 0.016677\n","Training accuracy: 99.8%\n","Validation accuracy: 79.2%\n","Loss at step 400: 0.010276\n","Training accuracy: 99.8%\n","Validation accuracy: 79.7%\n","Loss at step 500: 0.008914\n","Training accuracy: 99.9%\n","Validation accuracy: 79.8%\n","Loss at step 600: 0.009823\n","Training accuracy: 99.9%\n","Validation accuracy: 79.8%\n","Loss at step 700: 0.008193\n","Training accuracy: 99.9%\n","Validation accuracy: 79.8%\n","Loss at step 800: 0.022379\n","Training accuracy: 99.9%\n","Validation accuracy: 79.9%\n","Test accuracy: 87.0%\n"],"name":"stdout"}]}]}