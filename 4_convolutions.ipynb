{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_convolutions.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"4embtkV0pNxM","colab_type":"text"},"cell_type":"markdown","source":["Deep Learning\n","=============\n","\n","Assignment 4\n","------------\n","\n","Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n","\n","The goal of this assignment is make the neural network convolutional."]},{"metadata":{"id":"fP3_86MYRgC9","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!kill -9 -1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tm2CQN_Cpwj0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["# These are all the modules we'll be using later. Make sure you can import them\n","# before proceeding further.\n","from __future__ import print_function\n","import numpy as np\n","import tensorflow as tf\n","from six.moves import cPickle as pickle\n","from six.moves import range"],"execution_count":0,"outputs":[]},{"metadata":{"id":"-_FxUuvUPmSM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kRRC5817aaPB","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!google-drive-ocamlfuse -id 88281805073-h1u5lkmkjdv8lgrgioputno360hb5keq.apps.googleusercontent.com -secret F582cVamJ7P3GjTMHTeEGbub"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RipKy_PoP9S2","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["!mkdir drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5hEijaXlQQiU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":61},"outputId":"43779037-9765-4332-d395-e1babb759e30","executionInfo":{"status":"ok","timestamp":1523308406731,"user_tz":-180,"elapsed":1741,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["!pwd && ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content\r\n","datalab  drive\r\n"],"name":"stdout"}]},{"metadata":{"id":"y3-cj1bpmuxc","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":79},"cellView":"both","outputId":"ca52f59d-0dbf-439d-da45-449908839f07","executionInfo":{"status":"ok","timestamp":1523308436177,"user_tz":-180,"elapsed":17052,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["pickle_file = 'drive/ColabUtils/udacity/notMNIST.pickle'\n","\n","with open(pickle_file, 'rb') as f:\n","  save = pickle.load(f)\n","  train_dataset = save['train_dataset']\n","  train_labels = save['train_labels']\n","  valid_dataset = save['valid_dataset']\n","  valid_labels = save['valid_labels']\n","  test_dataset = save['test_dataset']\n","  test_labels = save['test_labels']\n","  del save  # hint to help gc free up memory\n","  print('Training set', train_dataset.shape, train_labels.shape)\n","  print('Validation set', valid_dataset.shape, valid_labels.shape)\n","  print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28) (200000,)\n","Validation set (10000, 28, 28) (10000,)\n","Test set (10000, 28, 28) (10000,)\n"],"name":"stdout"}]},{"metadata":{"id":"L7aHrm6nGDMB","colab_type":"text"},"cell_type":"markdown","source":["Reformat into a TensorFlow-friendly shape:\n","- convolutions need the image data formatted as a cube (width by height by #channels)\n","- labels as float 1-hot encodings."]},{"metadata":{"id":"IRSyYiIIGIzS","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":79},"cellView":"both","outputId":"41413da5-b058-4457-ed10-ce0345905aa6","executionInfo":{"status":"ok","timestamp":1523308507373,"user_tz":-180,"elapsed":1187,"user":{"displayName":"Дмитрий Люосев","photoUrl":"//lh4.googleusercontent.com/-OZZ_vVSd_4U/AAAAAAAAAAI/AAAAAAAAOJ0/wk1XtxptfDo/s50-c-k-no/photo.jpg","userId":"103317850580233450342"}}},"cell_type":"code","source":["image_size = 28\n","num_labels = 10\n","num_channels = 1 # grayscale\n","\n","import numpy as np\n","\n","def reformat(dataset, labels):\n","  dataset = dataset.reshape(\n","    (-1, image_size, image_size, num_channels)).astype(np.float32)\n","  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n","  return dataset, labels\n","train_dataset, train_labels = reformat(train_dataset, train_labels)\n","valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n","test_dataset, test_labels = reformat(test_dataset, test_labels)\n","print('Training set', train_dataset.shape, train_labels.shape)\n","print('Validation set', valid_dataset.shape, valid_labels.shape)\n","print('Test set', test_dataset.shape, test_labels.shape)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training set (200000, 28, 28, 1) (200000, 10)\n","Validation set (10000, 28, 28, 1) (10000, 10)\n","Test set (10000, 28, 28, 1) (10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"AgQDIREv02p1","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["def accuracy(predictions, labels):\n","  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n","          / predictions.shape[0])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5rhgjmROXu2O","colab_type":"text"},"cell_type":"markdown","source":["Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."]},{"metadata":{"id":"IZYv70SvvOan","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both"},"cell_type":"code","source":["batch_size = 16\n","patch_size = 5\n","depth = 16\n","num_hidden = 64\n","\n","graph = tf.Graph()\n","\n","with graph.as_default():\n","\n","  # Input data.\n","  tf_train_dataset = tf.placeholder(\n","    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n","  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n","  tf_valid_dataset = tf.constant(valid_dataset)\n","  tf_test_dataset = tf.constant(test_dataset)\n","  \n","  # Variables.\n","  layer1_weights = tf.Variable(tf.truncated_normal(\n","      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n","  layer1_biases = tf.Variable(tf.zeros([depth]))\n","  layer2_weights = tf.Variable(tf.truncated_normal(\n","      [patch_size, patch_size, depth, depth], stddev=0.1))\n","  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n","  layer3_weights = tf.Variable(tf.truncated_normal(\n","      [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n","  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n","  layer4_weights = tf.Variable(tf.truncated_normal(\n","      [num_hidden, num_labels], stddev=0.1))\n","  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n","  \n","  # Model.\n","  def model(data):\n","    conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n","    hidden = tf.nn.relu(conv + layer1_biases)\n","    conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n","    hidden = tf.nn.relu(conv + layer2_biases)\n","    shape = hidden.get_shape().as_list()\n","    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n","    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n","    return tf.matmul(hidden, layer4_weights) + layer4_biases\n","  \n","  # Training computation.\n","  logits = model(tf_train_dataset)\n","  loss = tf.reduce_mean(\n","    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n","    \n","  # Optimizer.\n","  optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n","  \n","  # Predictions for the training, validation, and test data.\n","  train_prediction = tf.nn.softmax(logits)\n","  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n","  test_prediction = tf.nn.softmax(model(tf_test_dataset))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"noKFb2UovVFR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}},"cellView":"both","outputId":"28941338-2ef9-4088-8bd1-44295661e628","executionInfo":{"elapsed":63292,"status":"ok","timestamp":1446658966251,"user":{"color":"","displayName":"","isAnonymous":false,"isMe":true,"permissionId":"","photoUrl":"","sessionId":"0","userId":""},"user_tz":480}},"cell_type":"code","source":["num_steps = 1001\n","\n","with tf.Session(graph=graph) as session:\n","  tf.global_variables_initializer().run()\n","  print('Initialized')\n","  for step in range(num_steps):\n","    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n","    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n","    batch_labels = train_labels[offset:(offset + batch_size), :]\n","    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n","    _, l, predictions = session.run(\n","      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n","    if (step % 50 == 0):\n","      print('Minibatch loss at step %d: %f' % (step, l))\n","      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n","      print('Validation accuracy: %.1f%%' % accuracy(\n","        valid_prediction.eval(), valid_labels))\n","  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initialized\n","Minibatch loss at step 0 : 3.51275\n","Minibatch accuracy: 6.2%\n","Validation accuracy: 12.8%\n","Minibatch loss at step 50 : 1.48703\n","Minibatch accuracy: 43.8%\n","Validation accuracy: 50.4%\n","Minibatch loss at step 100 : 1.04377\n","Minibatch accuracy: 68.8%\n","Validation accuracy: 67.4%\n","Minibatch loss at step 150 : 0.601682\n","Minibatch accuracy: 68.8%\n","Validation accuracy: 73.0%\n","Minibatch loss at step 200 : 0.898649\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 77.8%\n","Minibatch loss at step 250 : 1.3637\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 75.4%\n","Minibatch loss at step 300 : 1.41968\n","Minibatch accuracy: 62.5%\n","Validation accuracy: 76.0%\n","Minibatch loss at step 350 : 0.300648\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 80.2%\n","Minibatch loss at step 400 : 1.32092\n","Minibatch accuracy: 56.2%\n","Validation accuracy: 80.4%\n","Minibatch loss at step 450 : 0.556701\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 79.4%\n","Minibatch loss at step 500 : 1.65595\n","Minibatch accuracy: 43.8%\n","Validation accuracy: 79.6%\n","Minibatch loss at step 550 : 1.06995\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 81.2%\n","Minibatch loss at step 600 : 0.223684\n","Minibatch accuracy: 100.0%\n","Validation accuracy: 82.3%\n","Minibatch loss at step 650 : 0.619602\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 81.8%\n","Minibatch loss at step 700 : 0.812091\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 82.4%\n","Minibatch loss at step 750 : 0.276302\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 82.3%\n","Minibatch loss at step 800 : 0.450241\n","Minibatch accuracy: 81.2%\n","Validation accuracy: 82.3%\n","Minibatch loss at step 850 : 0.137139\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 82.3%\n","Minibatch loss at step 900 : 0.52664\n","Minibatch accuracy: 75.0%\n","Validation accuracy: 82.2%\n","Minibatch loss at step 950 : 0.623835\n","Minibatch accuracy: 87.5%\n","Validation accuracy: 82.1%\n","Minibatch loss at step 1000 : 0.243114\n","Minibatch accuracy: 93.8%\n","Validation accuracy: 82.9%\n","Test accuracy: 90.0%\n"],"name":"stdout"}]},{"metadata":{"id":"KedKkn4EutIK","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem 1\n","---------\n","\n","The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n","\n","---"]},{"metadata":{"id":"klf21gpbAgb-","colab_type":"text"},"cell_type":"markdown","source":["---\n","Problem 2\n","---------\n","\n","Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n","\n","---"]}]}